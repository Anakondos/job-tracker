#!/usr/bin/env python3
"""
Data Cleanup Tool ‚Äî —á–∏—Å—Ç–∫–∞ data/companies.json

–ò—Å–ø—Ä–∞–≤–ª—è–µ—Ç:
- –ë–∏—Ç—ã–µ URL (–æ–ø–µ—á–∞—Ç–∫–∏ –≤ board_url)
- –î—É–±–ª–∏ –∫–æ–º–ø–∞–Ω–∏–π (disable –¥—É–±–ª—å, –æ—Å—Ç–∞–≤–ª—è–µ–º –æ—Å–Ω–æ–≤–Ω—É—é)
- –ú—É—Å–æ—Ä–Ω—ã–µ –∑–∞–ø–∏—Å–∏ (–Ω–µ –∫–æ–º–ø–∞–Ω–∏–∏)
- –ö–æ–º–ø–∞–Ω–∏–∏ —Å –ø–æ—Å—Ç–æ—è–Ω–Ω—ã–º–∏ 404 –æ—à–∏–±–∫–∞–º–∏

–ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ:
    python tools/data_cleanup.py --dry-run      # –ø—Ä–µ–¥–ø—Ä–æ—Å–º–æ—Ç—Ä –∏–∑–º–µ–Ω–µ–Ω–∏–π
    python tools/data_cleanup.py                # –ø—Ä–∏–º–µ–Ω–∏—Ç—å –∏–∑–º–µ–Ω–µ–Ω–∏—è
    python tools/data_cleanup.py --triage-universal  # —Ç—Ä–∏–∞–∂ universal-–∫–æ–º–ø–∞–Ω–∏–π
"""

import json
import sys
import requests
from pathlib import Path
from datetime import datetime

PROJECT_ROOT = Path(__file__).parent.parent
COMPANIES_FILE = PROJECT_ROOT / "data" / "companies.json"
STATUS_FILE = PROJECT_ROOT / "data" / "company_status.json"


def load_companies() -> list:
    """–ó–∞–≥—Ä—É–∂–∞–µ–º companies.json"""
    with open(COMPANIES_FILE, "r", encoding="utf-8") as f:
        return json.load(f)


def save_companies(companies: list):
    """–°–æ—Ö—Ä–∞–Ω—è–µ–º companies.json"""
    with open(COMPANIES_FILE, "w", encoding="utf-8") as f:
        json.dump(companies, f, indent=2, ensure_ascii=False)
    print(f"‚úÖ –°–æ—Ö—Ä–∞–Ω–µ–Ω–æ {len(companies)} –∫–æ–º–ø–∞–Ω–∏–π –≤ {COMPANIES_FILE}")


def load_status() -> dict:
    """–ó–∞–≥—Ä—É–∂–∞–µ–º company_status.json"""
    if not STATUS_FILE.exists():
        return {}
    with open(STATUS_FILE, "r", encoding="utf-8") as f:
        return json.load(f)


def verify_url(url: str, timeout: int = 10) -> bool:
    """–ü—Ä–æ–≤–µ—Ä—è–µ–º –¥–æ—Å—Ç—É–ø–Ω–æ—Å—Ç—å URL (HTTP 200)"""
    try:
        # –î–ª—è greenhouse API
        if "boards.greenhouse.io" in url:
            token = url.rstrip("/").split("/")[-1]
            api_url = f"https://boards-api.greenhouse.io/v1/boards/{token}/jobs"
            r = requests.get(api_url, timeout=timeout)
            return r.status_code == 200
        # –î–ª—è lever
        if "jobs.lever.co" in url:
            slug = url.rstrip("/").split("/")[-1]
            api_url = f"https://api.lever.co/v0/postings/{slug}?mode=json"
            r = requests.get(api_url, timeout=timeout)
            return r.status_code == 200
        # –û–±—â–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞
        r = requests.head(url, timeout=timeout, allow_redirects=True)
        return r.status_code < 400
    except Exception as e:
        print(f"  ‚ö†Ô∏è –û—à–∏–±–∫–∞ –ø—Ä–æ–≤–µ—Ä–∫–∏ {url}: {e}")
        return False


# ===== –ü—Ä–∞–≤–∏–ª–∞ –∏—Å–ø—Ä–∞–≤–ª–µ–Ω–∏—è =====

# a) –ë–∏—Ç—ã–µ URL ‚Äî {id: {field: new_value}}
URL_FIXES = {
    "insightsoftware": {
        "board_url": "https://boards.greenhouse.io/insightsoftware",
    },
}

# b) –î—É–±–ª–∏ ‚Äî disable –¥—É–±–ª—å, –æ—Å—Ç–∞–≤–ª—è–µ–º –æ—Å–Ω–æ–≤–Ω—É—é
#    {duplicate_id: reason}
DUPLICATES_TO_DISABLE = {
    "wellsfargo": "–î—É–±–ª—å wells_fargo (workday)",
    "apply": "–î—É–±–ª—å deloitte (–æ–±–∞ ‚Üí apply.deloitte.com)",
    "external-firstcitizens": "–î—É–±–ª—å firstcitizens (icims)",
}

# c) –ú—É—Å–æ—Ä–Ω—ã–µ –∑–∞–ø–∏—Å–∏ ‚Äî –Ω–µ —è–≤–ª—è—é—Ç—Å—è –∫–æ–º–ø–∞–Ω–∏—è–º–∏
JUNK_ENTRIES = {
    "boards": "boards.greenhouse.io ‚Äî –Ω–µ –∫–æ–º–ø–∞–Ω–∏—è, –∞ –ø–ª–∞—Ç—Ñ–æ—Ä–º–∞",
    "job-boards": "job-boards.greenhouse.io ‚Äî –Ω–µ –∫–æ–º–ø–∞–Ω–∏—è",
    "wd1": "wd1.myworkdaysite.com ‚Äî –Ω–µ –∫–æ–º–ø–∞–Ω–∏—è, –∞ –ø–ª–∞—Ç—Ñ–æ—Ä–º–∞",
    # ashbyhq —É–∂–µ disabled
}

# d) –ö–æ–º–ø–∞–Ω–∏–∏ —Å –ø–æ—Å—Ç–æ—è–Ω–Ω—ã–º–∏ 404
BROKEN_COMPANIES = {
    "k4connect": "ats_404",
    "fmsystems": "ats_404",
    "analytics8": "ats_404",
    "protect-ai": "ats_404",
    "kevel": "ats_404",
    "imaginovation": "ats_404",
}

# e) –ö–æ–º–ø–∞–Ω–∏–∏ –¥–ª—è —Ä–∞—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏—è (—Ä–∞–Ω–µ–µ —Ä–∞–±–æ—Ç–∞–ª–∏, —Ç–µ–ø–µ—Ä—å –æ—à–∏–±–∫–∏)
NEEDS_INVESTIGATION = {
    "bosch": "SmartRecruiters JSON parse error",
    "visa": "SmartRecruiters JSON parse error",
}


def fix_urls(companies: list, dry_run: bool) -> int:
    """–ò—Å–ø—Ä–∞–≤–ª—è–µ–º –æ–ø–µ—á–∞—Ç–∫–∏ –≤ URL"""
    changes = 0
    for c in companies:
        cid = c.get("id")
        if cid in URL_FIXES:
            for field, new_value in URL_FIXES[cid].items():
                old_value = c.get(field)
                if old_value != new_value:
                    print(f"  üîß [{cid}] {field}: '{old_value}' ‚Üí '{new_value}'")
                    if not dry_run:
                        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–æ–≤—ã–π URL –ø–µ—Ä–µ–¥ –ø—Ä–∏–º–µ–Ω–µ–Ω–∏–µ–º
                        if field == "board_url" and not verify_url(new_value):
                            print(f"    ‚ùå –ù–æ–≤—ã–π URL –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω, –ø—Ä–æ–ø—É—Å–∫–∞–µ–º!")
                            continue
                        c[field] = new_value
                    changes += 1
    return changes


def disable_duplicates(companies: list, dry_run: bool) -> int:
    """Disable –¥—É–±–ª–µ–π –∫–æ–º–ø–∞–Ω–∏–π"""
    changes = 0
    for c in companies:
        cid = c.get("id")
        if cid in DUPLICATES_TO_DISABLE and c.get("enabled", True):
            reason = DUPLICATES_TO_DISABLE[cid]
            print(f"  üîÑ [{cid}] enabled ‚Üí false (–ø—Ä–∏—á–∏–Ω–∞: {reason})")
            if not dry_run:
                c["enabled"] = False
                c["status"] = "duplicate"
            changes += 1
    return changes


def disable_junk(companies: list, dry_run: bool) -> int:
    """Disable –º—É—Å–æ—Ä–Ω—ã—Ö –∑–∞–ø–∏—Å–µ–π"""
    changes = 0
    for c in companies:
        cid = c.get("id")
        if cid in JUNK_ENTRIES and c.get("enabled", True):
            reason = JUNK_ENTRIES[cid]
            print(f"  üóëÔ∏è [{cid}] enabled ‚Üí false (–ø—Ä–∏—á–∏–Ω–∞: {reason})")
            if not dry_run:
                c["enabled"] = False
                c["status"] = "not_a_company"
            changes += 1
    return changes


def disable_broken(companies: list, dry_run: bool) -> int:
    """Disable –∫–æ–º–ø–∞–Ω–∏–π —Å –ø–æ—Å—Ç–æ—è–Ω–Ω—ã–º–∏ 404"""
    changes = 0
    for c in companies:
        cid = c.get("id")
        if cid in BROKEN_COMPANIES and c.get("enabled", True):
            new_status = BROKEN_COMPANIES[cid]
            print(f"  ‚ùå [{cid}] enabled ‚Üí false, status ‚Üí {new_status}")
            if not dry_run:
                c["enabled"] = False
                c["status"] = new_status
            changes += 1
    return changes


def mark_investigation(companies: list, dry_run: bool) -> int:
    """–ü–æ–º–µ—á–∞–µ–º –∫–æ–º–ø–∞–Ω–∏–∏ –¥–ª—è —Ä–∞—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏—è"""
    changes = 0
    for c in companies:
        cid = c.get("id")
        if cid in NEEDS_INVESTIGATION and c.get("status") != "needs_investigation":
            reason = NEEDS_INVESTIGATION[cid]
            print(f"  üîç [{cid}] status ‚Üí needs_investigation ({reason})")
            if not dry_run:
                c["status"] = "needs_investigation"
            changes += 1
    return changes


def print_summary(companies: list):
    """–ü–µ—á–∞—Ç–∞–µ–º —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É"""
    total = len(companies)
    enabled = sum(1 for c in companies if c.get("enabled", True))
    disabled = total - enabled
    no_tags = sum(1 for c in companies if not c.get("tags") and c.get("enabled", True))
    no_industry = sum(1 for c in companies if not c.get("industry") and c.get("enabled", True))
    universal = sum(1 for c in companies if c.get("ats") == "universal" and c.get("enabled", True))

    print(f"\nüìä –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞:")
    print(f"  –í—Å–µ–≥–æ –∫–æ–º–ø–∞–Ω–∏–π: {total}")
    print(f"  Enabled: {enabled}")
    print(f"  Disabled: {disabled}")
    print(f"  –ë–µ–∑ —Ç–µ–≥–æ–≤ (enabled): {no_tags}")
    print(f"  –ë–µ–∑ industry (enabled): {no_industry}")
    print(f"  ATS=universal (enabled): {universal}")


def main():
    dry_run = "--dry-run" in sys.argv
    triage = "--triage-universal" in sys.argv

    if triage:
        # Phase 1.3 ‚Äî —Ç—Ä–∏–∞–∂ universal-–∫–æ–º–ø–∞–Ω–∏–π
        print("\nüîÑ –¢—Ä–∏–∞–∂ universal-–∫–æ–º–ø–∞–Ω–∏–π")
        print("=" * 50)
        print("‚ö†Ô∏è –≠—Ç–∞ —Ñ—É–Ω–∫—Ü–∏—è –±—É–¥–µ—Ç —Ä–µ–∞–ª–∏–∑–æ–≤–∞–Ω–∞ –≤ –®–∞–≥–µ 1.3")
        return

    mode = "DRY RUN" if dry_run else "APPLY"
    print(f"\nüßπ Data Cleanup Tool [{mode}]")
    print("=" * 50)

    companies = load_companies()
    print(f"üìÇ –ó–∞–≥—Ä—É–∂–µ–Ω–æ {len(companies)} –∫–æ–º–ø–∞–Ω–∏–π")

    total_changes = 0

    print(f"\n--- a) –ò—Å–ø—Ä–∞–≤–ª–µ–Ω–∏–µ –±–∏—Ç—ã—Ö URL ---")
    total_changes += fix_urls(companies, dry_run)

    print(f"\n--- b) Disable –¥—É–±–ª–µ–π ---")
    total_changes += disable_duplicates(companies, dry_run)

    print(f"\n--- c) Disable –º—É—Å–æ—Ä–Ω—ã—Ö –∑–∞–ø–∏—Å–µ–π ---")
    total_changes += disable_junk(companies, dry_run)

    print(f"\n--- d) Disable —Å–ª–æ–º–∞–Ω–Ω—ã—Ö –∫–æ–º–ø–∞–Ω–∏–π (404) ---")
    total_changes += disable_broken(companies, dry_run)

    print(f"\n--- e) –ü–æ–º–µ—Ç–∏—Ç—å –¥–ª—è —Ä–∞—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏—è ---")
    total_changes += mark_investigation(companies, dry_run)

    print(f"\n{'=' * 50}")
    print(f"üìù –í—Å–µ–≥–æ –∏–∑–º–µ–Ω–µ–Ω–∏–π: {total_changes}")

    if dry_run:
        print(f"\n‚ö†Ô∏è DRY RUN ‚Äî –∏–∑–º–µ–Ω–µ–Ω–∏—è –ù–ï —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã")
        print(f"–ó–∞–ø—É—Å—Ç–∏—Ç–µ –±–µ–∑ --dry-run –¥–ª—è –ø—Ä–∏–º–µ–Ω–µ–Ω–∏—è")
    elif total_changes > 0:
        save_companies(companies)
    else:
        print("–ù–µ—Ç –∏–∑–º–µ–Ω–µ–Ω–∏–π –¥–ª—è –ø—Ä–∏–º–µ–Ω–µ–Ω–∏—è")

    # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É –ø–æ—Å–ª–µ –∏–∑–º–µ–Ω–µ–Ω–∏–π
    if not dry_run and total_changes > 0:
        companies = load_companies()  # –ø–µ—Ä–µ—á–∏—Ç—ã–≤–∞–µ–º
    print_summary(companies)


if __name__ == "__main__":
    main()
