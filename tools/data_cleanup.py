#!/usr/bin/env python3
"""
Data Cleanup Tool ‚Äî —á–∏—Å—Ç–∫–∞ data/companies.json

–ò—Å–ø—Ä–∞–≤–ª—è–µ—Ç:
- –ë–∏—Ç—ã–µ URL (–æ–ø–µ—á–∞—Ç–∫–∏ –≤ board_url)
- –î—É–±–ª–∏ –∫–æ–º–ø–∞–Ω–∏–π (disable –¥—É–±–ª—å, –æ—Å—Ç–∞–≤–ª—è–µ–º –æ—Å–Ω–æ–≤–Ω—É—é)
- –ú—É—Å–æ—Ä–Ω—ã–µ –∑–∞–ø–∏—Å–∏ (–Ω–µ –∫–æ–º–ø–∞–Ω–∏–∏)
- –ö–æ–º–ø–∞–Ω–∏–∏ —Å –ø–æ—Å—Ç–æ—è–Ω–Ω—ã–º–∏ 404 –æ—à–∏–±–∫–∞–º–∏

–ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ:
    python tools/data_cleanup.py --dry-run      # –ø—Ä–µ–¥–ø—Ä–æ—Å–º–æ—Ç—Ä –∏–∑–º–µ–Ω–µ–Ω–∏–π
    python tools/data_cleanup.py                # –ø—Ä–∏–º–µ–Ω–∏—Ç—å –∏–∑–º–µ–Ω–µ–Ω–∏—è
    python tools/data_cleanup.py --triage-universal  # —Ç—Ä–∏–∞–∂ universal-–∫–æ–º–ø–∞–Ω–∏–π
"""

import json
import re
import sys
import requests
from pathlib import Path
from datetime import datetime
from urllib.parse import urlparse

PROJECT_ROOT = Path(__file__).parent.parent
COMPANIES_FILE = PROJECT_ROOT / "data" / "companies.json"
STATUS_FILE = PROJECT_ROOT / "data" / "company_status.json"


def load_companies() -> list:
    """–ó–∞–≥—Ä—É–∂–∞–µ–º companies.json"""
    with open(COMPANIES_FILE, "r", encoding="utf-8") as f:
        return json.load(f)


def save_companies(companies: list):
    """–°–æ—Ö—Ä–∞–Ω—è–µ–º companies.json"""
    with open(COMPANIES_FILE, "w", encoding="utf-8") as f:
        json.dump(companies, f, indent=2, ensure_ascii=False)
    print(f"‚úÖ –°–æ—Ö—Ä–∞–Ω–µ–Ω–æ {len(companies)} –∫–æ–º–ø–∞–Ω–∏–π –≤ {COMPANIES_FILE}")


def load_status() -> dict:
    """–ó–∞–≥—Ä—É–∂–∞–µ–º company_status.json"""
    if not STATUS_FILE.exists():
        return {}
    with open(STATUS_FILE, "r", encoding="utf-8") as f:
        return json.load(f)


def verify_url(url: str, timeout: int = 10) -> bool:
    """–ü—Ä–æ–≤–µ—Ä—è–µ–º –¥–æ—Å—Ç—É–ø–Ω–æ—Å—Ç—å URL (HTTP 200)"""
    try:
        # –î–ª—è greenhouse API
        if "boards.greenhouse.io" in url:
            token = url.rstrip("/").split("/")[-1]
            api_url = f"https://boards-api.greenhouse.io/v1/boards/{token}/jobs"
            r = requests.get(api_url, timeout=timeout)
            return r.status_code == 200
        # –î–ª—è lever
        if "jobs.lever.co" in url:
            slug = url.rstrip("/").split("/")[-1]
            api_url = f"https://api.lever.co/v0/postings/{slug}?mode=json"
            r = requests.get(api_url, timeout=timeout)
            return r.status_code == 200
        # –û–±—â–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞
        r = requests.head(url, timeout=timeout, allow_redirects=True)
        return r.status_code < 400
    except Exception as e:
        print(f"  ‚ö†Ô∏è –û—à–∏–±–∫–∞ –ø—Ä–æ–≤–µ—Ä–∫–∏ {url}: {e}")
        return False


# ===== –ü—Ä–∞–≤–∏–ª–∞ –∏—Å–ø—Ä–∞–≤–ª–µ–Ω–∏—è =====

# a) –ë–∏—Ç—ã–µ URL ‚Äî {id: {field: new_value}}
URL_FIXES = {
    "insightsoftware": {
        "board_url": "https://boards.greenhouse.io/insightsoftware",
    },
}

# b) –î—É–±–ª–∏ ‚Äî disable –¥—É–±–ª—å, –æ—Å—Ç–∞–≤–ª—è–µ–º –æ—Å–Ω–æ–≤–Ω—É—é
#    {duplicate_id: reason}
DUPLICATES_TO_DISABLE = {
    "wellsfargo": "–î—É–±–ª—å wells_fargo (workday)",
    "apply": "–î—É–±–ª—å deloitte (–æ–±–∞ ‚Üí apply.deloitte.com)",
    "external-firstcitizens": "–î—É–±–ª—å firstcitizens (icims)",
}

# c) –ú—É—Å–æ—Ä–Ω—ã–µ –∑–∞–ø–∏—Å–∏ ‚Äî –Ω–µ —è–≤–ª—è—é—Ç—Å—è –∫–æ–º–ø–∞–Ω–∏—è–º–∏
JUNK_ENTRIES = {
    "boards": "boards.greenhouse.io ‚Äî –Ω–µ –∫–æ–º–ø–∞–Ω–∏—è, –∞ –ø–ª–∞—Ç—Ñ–æ—Ä–º–∞",
    "job-boards": "job-boards.greenhouse.io ‚Äî –Ω–µ –∫–æ–º–ø–∞–Ω–∏—è",
    "wd1": "wd1.myworkdaysite.com ‚Äî –Ω–µ –∫–æ–º–ø–∞–Ω–∏—è, –∞ –ø–ª–∞—Ç—Ñ–æ—Ä–º–∞",
    # ashbyhq —É–∂–µ disabled
}

# d) –ö–æ–º–ø–∞–Ω–∏–∏ —Å –ø–æ—Å—Ç–æ—è–Ω–Ω—ã–º–∏ 404
BROKEN_COMPANIES = {
    "k4connect": "ats_404",
    "fmsystems": "ats_404",
    "analytics8": "ats_404",
    "protect-ai": "ats_404",
    "kevel": "ats_404",
    "imaginovation": "ats_404",
}

# e) –ö–æ–º–ø–∞–Ω–∏–∏ –¥–ª—è —Ä–∞—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏—è (—Ä–∞–Ω–µ–µ —Ä–∞–±–æ—Ç–∞–ª–∏, —Ç–µ–ø–µ—Ä—å –æ—à–∏–±–∫–∏)
NEEDS_INVESTIGATION = {
    "bosch": "SmartRecruiters JSON parse error",
    "visa": "SmartRecruiters JSON parse error",
}


def fix_urls(companies: list, dry_run: bool) -> int:
    """–ò—Å–ø—Ä–∞–≤–ª—è–µ–º –æ–ø–µ—á–∞—Ç–∫–∏ –≤ URL"""
    changes = 0
    for c in companies:
        cid = c.get("id")
        if cid in URL_FIXES:
            for field, new_value in URL_FIXES[cid].items():
                old_value = c.get(field)
                if old_value != new_value:
                    print(f"  üîß [{cid}] {field}: '{old_value}' ‚Üí '{new_value}'")
                    if not dry_run:
                        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–æ–≤—ã–π URL –ø–µ—Ä–µ–¥ –ø—Ä–∏–º–µ–Ω–µ–Ω–∏–µ–º
                        if field == "board_url" and not verify_url(new_value):
                            print(f"    ‚ùå –ù–æ–≤—ã–π URL –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω, –ø—Ä–æ–ø—É—Å–∫–∞–µ–º!")
                            continue
                        c[field] = new_value
                    changes += 1
    return changes


def disable_duplicates(companies: list, dry_run: bool) -> int:
    """Disable –¥—É–±–ª–µ–π –∫–æ–º–ø–∞–Ω–∏–π"""
    changes = 0
    for c in companies:
        cid = c.get("id")
        if cid in DUPLICATES_TO_DISABLE and c.get("enabled", True):
            reason = DUPLICATES_TO_DISABLE[cid]
            print(f"  üîÑ [{cid}] enabled ‚Üí false (–ø—Ä–∏—á–∏–Ω–∞: {reason})")
            if not dry_run:
                c["enabled"] = False
                c["status"] = "duplicate"
            changes += 1
    return changes


def disable_junk(companies: list, dry_run: bool) -> int:
    """Disable –º—É—Å–æ—Ä–Ω—ã—Ö –∑–∞–ø–∏—Å–µ–π"""
    changes = 0
    for c in companies:
        cid = c.get("id")
        if cid in JUNK_ENTRIES and c.get("enabled", True):
            reason = JUNK_ENTRIES[cid]
            print(f"  üóëÔ∏è [{cid}] enabled ‚Üí false (–ø—Ä–∏—á–∏–Ω–∞: {reason})")
            if not dry_run:
                c["enabled"] = False
                c["status"] = "not_a_company"
            changes += 1
    return changes


def disable_broken(companies: list, dry_run: bool) -> int:
    """Disable –∫–æ–º–ø–∞–Ω–∏–π —Å –ø–æ—Å—Ç–æ—è–Ω–Ω—ã–º–∏ 404"""
    changes = 0
    for c in companies:
        cid = c.get("id")
        if cid in BROKEN_COMPANIES and c.get("enabled", True):
            new_status = BROKEN_COMPANIES[cid]
            print(f"  ‚ùå [{cid}] enabled ‚Üí false, status ‚Üí {new_status}")
            if not dry_run:
                c["enabled"] = False
                c["status"] = new_status
            changes += 1
    return changes


def mark_investigation(companies: list, dry_run: bool) -> int:
    """–ü–æ–º–µ—á–∞–µ–º –∫–æ–º–ø–∞–Ω–∏–∏ –¥–ª—è —Ä–∞—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏—è"""
    changes = 0
    for c in companies:
        cid = c.get("id")
        if cid in NEEDS_INVESTIGATION and c.get("status") != "needs_investigation":
            reason = NEEDS_INVESTIGATION[cid]
            print(f"  üîç [{cid}] status ‚Üí needs_investigation ({reason})")
            if not dry_run:
                c["status"] = "needs_investigation"
            changes += 1
    return changes


# ===== –ü–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ–º—ã–µ ATS (–∏–∑ main.py ATS_PARSERS) =====
SUPPORTED_ATS = ["greenhouse", "lever", "smartrecruiters", "ashby", "workday", "atlassian", "phenom"]


def detect_ats_from_board_url(board_url: str) -> dict | None:
    """
    –û–ø—Ä–µ–¥–µ–ª—è–µ–º ATS –ø–æ board_url –∫–æ–º–ø–∞–Ω–∏–∏.
    –£–ø—Ä–æ—â—ë–Ω–Ω–∞—è –≤–µ—Ä—Å–∏—è detect_ats_from_url() –∏–∑ main.py ‚Äî —Ä–∞–±–æ—Ç–∞–µ—Ç —Å board_url, –Ω–µ —Å job URL.
    –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç {ats, board_url} –µ—Å–ª–∏ –æ–ø—Ä–µ–¥–µ–ª—ë–Ω –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ–º—ã–π ATS, –∏–Ω–∞—á–µ None.
    """
    if not board_url:
        return None

    parsed = urlparse(board_url)
    host = parsed.netloc.lower()
    path = parsed.path

    # Greenhouse: boards.greenhouse.io/company
    if "greenhouse.io" in host:
        slug = path.strip("/").split("/")[0] if path.strip("/") else ""
        if slug:
            return {"ats": "greenhouse", "board_url": f"https://boards.greenhouse.io/{slug}"}

    # Lever: jobs.lever.co/company
    if "lever.co" in host:
        slug = path.strip("/").split("/")[0] if path.strip("/") else ""
        if slug:
            return {"ats": "lever", "board_url": f"https://jobs.lever.co/{slug}"}

    # SmartRecruiters: jobs.smartrecruiters.com/Company
    if "smartrecruiters.com" in host:
        slug = path.strip("/").split("/")[0] if path.strip("/") else ""
        if slug:
            return {"ats": "smartrecruiters", "board_url": f"https://jobs.smartrecruiters.com/{slug}"}

    # Ashby: jobs.ashbyhq.com/company
    if "ashbyhq.com" in host:
        slug = path.strip("/").split("/")[0] if path.strip("/") else ""
        if slug:
            return {"ats": "ashby", "board_url": f"https://jobs.ashbyhq.com/{slug}"}

    # Workday: company.wd1.myworkdayjobs.com/site
    if "myworkdayjobs.com" in host:
        parts = host.split(".")
        company = parts[0] if parts else ""
        wd_match = re.search(r"\.(wd\d+)\.", host)
        wd_num = wd_match.group(1) if wd_match else "wd1"
        site_match = re.search(r"myworkdayjobs\.com/(?:[a-z][a-z]-[A-Z][A-Z]/)?([^/]+)", board_url)
        site = site_match.group(1) if site_match else company
        return {"ats": "workday", "board_url": f"https://{company}.{wd_num}.myworkdayjobs.com/{site}"}

    # iCIMS: external-company.icims.com/jobs
    if "icims.com" in host:
        subdomain = host.split(".")[0]
        return {"ats": "icims", "board_url": f"https://{subdomain}.icims.com/jobs"}

    # Phenom: –æ–±—ã—á–Ω–æ company.wd1.myworkdayjobs.com –∏–ª–∏ careers.company.com —Å phenom JS
    # Phenom –æ–ø—Ä–µ–¥–µ–ª—è–µ—Ç—Å—è –ø–æ —Å–æ–¥–µ—Ä–∂–∏–º–æ–º—É, –∑–¥–µ—Å—å –Ω–µ –º–æ–∂–µ–º ‚Äî –ø—Ä–æ–ø—É—Å–∫–∞–µ–º

    return None


def try_detect_ats_via_http(board_url: str) -> dict | None:
    """
    –ü—Ä–æ–±—É–µ–º –æ–ø—Ä–µ–¥–µ–ª–∏—Ç—å ATS —á–µ—Ä–µ–∑ HTTP-–∑–∞–ø—Ä–æ—Å –∫ board_url.
    –ò—â–µ–º –ø–∞—Ç—Ç–µ—Ä–Ω—ã –≤ HTML/—Ä–µ–¥–∏—Ä–µ–∫—Ç–∞—Ö.
    """
    try:
        r = requests.get(board_url, timeout=15, allow_redirects=True, headers={
            "User-Agent": "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36"
        })
        final_url = r.url.lower()

        # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Ä–µ–¥–∏—Ä–µ–∫—Ç –Ω–∞ –∏–∑–≤–µ—Å—Ç–Ω—ã–π ATS
        redirect_result = detect_ats_from_board_url(final_url)
        if redirect_result and redirect_result["ats"] in SUPPORTED_ATS:
            return redirect_result

        # –ò—â–µ–º –ø–∞—Ç—Ç–µ—Ä–Ω—ã –≤ HTML
        html = r.text[:10000].lower()

        # Greenhouse embed
        if "boards.greenhouse.io" in html or "greenhouse.io/embed" in html:
            match = re.search(r"boards\.greenhouse\.io/([a-z0-9_-]+)", html)
            if match:
                slug = match.group(1)
                return {"ats": "greenhouse", "board_url": f"https://boards.greenhouse.io/{slug}"}

        # Lever embed
        if "jobs.lever.co" in html or "lever.co/embed" in html:
            match = re.search(r"jobs\.lever\.co/([a-z0-9_-]+)", html)
            if match:
                slug = match.group(1)
                return {"ats": "lever", "board_url": f"https://jobs.lever.co/{slug}"}

        # Ashby embed
        if "jobs.ashbyhq.com" in html:
            match = re.search(r"jobs\.ashbyhq\.com/([a-z0-9_-]+)", html)
            if match:
                slug = match.group(1)
                return {"ats": "ashby", "board_url": f"https://jobs.ashbyhq.com/{slug}"}

        # SmartRecruiters embed
        if "jobs.smartrecruiters.com" in html:
            match = re.search(r"jobs\.smartrecruiters\.com/([a-zA-Z0-9_-]+)", html)
            if match:
                slug = match.group(1)
                return {"ats": "smartrecruiters", "board_url": f"https://jobs.smartrecruiters.com/{slug}"}

    except Exception as e:
        print(f"    ‚ö†Ô∏è HTTP –æ—à–∏–±–∫–∞: {e}")

    return None


def triage_universal(companies: list, dry_run: bool) -> int:
    """
    –¢—Ä–∏–∞–∂ universal-–∫–æ–º–ø–∞–Ω–∏–π:
    1. –û–ø—Ä–µ–¥–µ–ª—è–µ–º ATS –ø–æ URL –ø–∞—Ç—Ç–µ—Ä–Ω—É (detect_ats_from_board_url)
    2. –ï—Å–ª–∏ –Ω–µ –Ω–∞—à–ª–∏ ‚Äî –ø—Ä–æ–±—É–µ–º HTTP –∑–∞–ø—Ä–æ—Å (try_detect_ats_via_http)
    3. –ï—Å–ª–∏ –Ω–∞—à–ª–∏ –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ–º—ã–π ATS ‚Äî –æ–±–Ω–æ–≤–ª—è–µ–º ats + board_url
    4. –ï—Å–ª–∏ –Ω–∞—à–ª–∏ –Ω–µ–ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ–º—ã–π (icims, zoho, etc.) ‚Äî –ø–æ–º–µ—á–∞–µ–º —Å—Ç–∞—Ç—É—Å
    5. –ï—Å–ª–∏ –Ω–µ –æ–ø—Ä–µ–¥–µ–ª–∏–ª–∏ ‚Äî disable + status="unsupported_ats"
    """
    changes = 0
    universal = [c for c in companies if c.get("ats") == "universal" and c.get("enabled", True)]

    print(f"\nüìã –ù–∞–π–¥–µ–Ω–æ {len(universal)} enabled universal-–∫–æ–º–ø–∞–Ω–∏–π\n")

    for c in universal:
        cid = c["id"]
        board_url = c.get("board_url", "")
        print(f"  [{cid}] {board_url}")

        # –®–∞–≥ 1: –æ–ø—Ä–µ–¥–µ–ª—è–µ–º ATS –ø–æ URL –ø–∞—Ç—Ç–µ—Ä–Ω—É
        result = detect_ats_from_board_url(board_url)

        # –®–∞–≥ 2: –µ—Å–ª–∏ –Ω–µ –Ω–∞—à–ª–∏ ‚Äî HTTP –∑–∞–ø—Ä–æ—Å
        if not result and not dry_run:
            print(f"    üîç –ü—Ä–æ–≤–µ—Ä—è–µ–º —á–µ—Ä–µ–∑ HTTP...")
            result = try_detect_ats_via_http(board_url)

        if result:
            new_ats = result["ats"]
            new_board_url = result["board_url"]

            if new_ats in SUPPORTED_ATS:
                # –ü–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ–º—ã–π ATS ‚Äî –æ–±–Ω–æ–≤–ª—è–µ–º
                print(f"    ‚úÖ ATS –æ–ø—Ä–µ–¥–µ–ª—ë–Ω: {new_ats} ‚Üí board_url: {new_board_url}")
                if not dry_run:
                    c["ats"] = new_ats
                    c["board_url"] = new_board_url
                changes += 1
            else:
                # –ù–µ–ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ–º—ã–π ATS (icims, etc.) ‚Äî –ø–æ–º–µ—á–∞–µ–º –Ω–æ –Ω–µ disable
                print(f"    üü° ATS –æ–ø—Ä–µ–¥–µ–ª—ë–Ω: {new_ats} (–Ω–µ –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç—Å—è)")
                if not dry_run:
                    c["ats"] = new_ats
                    c["board_url"] = new_board_url
                    c["status"] = "unsupported_ats"
                changes += 1
        else:
            # –ù–µ –æ–ø—Ä–µ–¥–µ–ª–∏–ª–∏ ATS ‚Äî disable
            if dry_run:
                print(f"    ‚ùì ATS –Ω–µ –æ–ø—Ä–µ–¥–µ–ª—ë–Ω (dry-run, HTTP –Ω–µ –≤—ã–∑—ã–≤–∞–µ—Ç—Å—è)")
            else:
                print(f"    ‚ùå ATS –Ω–µ –æ–ø—Ä–µ–¥–µ–ª—ë–Ω ‚Üí disable")
                c["enabled"] = False
                c["status"] = "unsupported_ats"
                changes += 1

    return changes


def print_summary(companies: list):
    """–ü–µ—á–∞—Ç–∞–µ–º —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É"""
    total = len(companies)
    enabled = sum(1 for c in companies if c.get("enabled", True))
    disabled = total - enabled
    no_tags = sum(1 for c in companies if not c.get("tags") and c.get("enabled", True))
    no_industry = sum(1 for c in companies if not c.get("industry") and c.get("enabled", True))
    universal = sum(1 for c in companies if c.get("ats") == "universal" and c.get("enabled", True))

    print(f"\nüìä –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞:")
    print(f"  –í—Å–µ–≥–æ –∫–æ–º–ø–∞–Ω–∏–π: {total}")
    print(f"  Enabled: {enabled}")
    print(f"  Disabled: {disabled}")
    print(f"  –ë–µ–∑ —Ç–µ–≥–æ–≤ (enabled): {no_tags}")
    print(f"  –ë–µ–∑ industry (enabled): {no_industry}")
    print(f"  ATS=universal (enabled): {universal}")


def main():
    dry_run = "--dry-run" in sys.argv
    triage = "--triage-universal" in sys.argv

    if triage:
        # Phase 1.3 ‚Äî —Ç—Ä–∏–∞–∂ universal-–∫–æ–º–ø–∞–Ω–∏–π
        mode_t = "DRY RUN" if dry_run else "APPLY"
        print(f"\nüîÑ –¢—Ä–∏–∞–∂ universal-–∫–æ–º–ø–∞–Ω–∏–π [{mode_t}]")
        print("=" * 50)

        companies = load_companies()
        print(f"üìÇ –ó–∞–≥—Ä—É–∂–µ–Ω–æ {len(companies)} –∫–æ–º–ø–∞–Ω–∏–π")

        changes = triage_universal(companies, dry_run)

        print(f"\n{'=' * 50}")
        print(f"üìù –ò–∑–º–µ–Ω–µ–Ω–æ: {changes} –∫–æ–º–ø–∞–Ω–∏–π")

        if dry_run:
            print("‚ö†Ô∏è DRY RUN ‚Äî –∏–∑–º–µ–Ω–µ–Ω–∏—è –ù–ï —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã")
            print("–ó–∞–ø—É—Å—Ç–∏—Ç–µ –±–µ–∑ --dry-run –¥–ª—è –ø—Ä–∏–º–µ–Ω–µ–Ω–∏—è:")
            print("  python tools/data_cleanup.py --triage-universal")
        elif changes > 0:
            save_companies(companies)

        print_summary(load_companies() if not dry_run and changes > 0 else companies)
        return

    mode = "DRY RUN" if dry_run else "APPLY"
    print(f"\nüßπ Data Cleanup Tool [{mode}]")
    print("=" * 50)

    companies = load_companies()
    print(f"üìÇ –ó–∞–≥—Ä—É–∂–µ–Ω–æ {len(companies)} –∫–æ–º–ø–∞–Ω–∏–π")

    total_changes = 0

    print(f"\n--- a) –ò—Å–ø—Ä–∞–≤–ª–µ–Ω–∏–µ –±–∏—Ç—ã—Ö URL ---")
    total_changes += fix_urls(companies, dry_run)

    print(f"\n--- b) Disable –¥—É–±–ª–µ–π ---")
    total_changes += disable_duplicates(companies, dry_run)

    print(f"\n--- c) Disable –º—É—Å–æ—Ä–Ω—ã—Ö –∑–∞–ø–∏—Å–µ–π ---")
    total_changes += disable_junk(companies, dry_run)

    print(f"\n--- d) Disable —Å–ª–æ–º–∞–Ω–Ω—ã—Ö –∫–æ–º–ø–∞–Ω–∏–π (404) ---")
    total_changes += disable_broken(companies, dry_run)

    print(f"\n--- e) –ü–æ–º–µ—Ç–∏—Ç—å –¥–ª—è —Ä–∞—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏—è ---")
    total_changes += mark_investigation(companies, dry_run)

    print(f"\n{'=' * 50}")
    print(f"üìù –í—Å–µ–≥–æ –∏–∑–º–µ–Ω–µ–Ω–∏–π: {total_changes}")

    if dry_run:
        print(f"\n‚ö†Ô∏è DRY RUN ‚Äî –∏–∑–º–µ–Ω–µ–Ω–∏—è –ù–ï —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã")
        print(f"–ó–∞–ø—É—Å—Ç–∏—Ç–µ –±–µ–∑ --dry-run –¥–ª—è –ø—Ä–∏–º–µ–Ω–µ–Ω–∏—è")
    elif total_changes > 0:
        save_companies(companies)
    else:
        print("–ù–µ—Ç –∏–∑–º–µ–Ω–µ–Ω–∏–π –¥–ª—è –ø—Ä–∏–º–µ–Ω–µ–Ω–∏—è")

    # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É –ø–æ—Å–ª–µ –∏–∑–º–µ–Ω–µ–Ω–∏–π
    if not dry_run and total_changes > 0:
        companies = load_companies()  # –ø–µ—Ä–µ—á–∏—Ç—ã–≤–∞–µ–º
    print_summary(companies)


if __name__ == "__main__":
    main()
